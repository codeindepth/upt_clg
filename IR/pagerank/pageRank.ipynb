{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found links in c.html: ['programming.html']\n",
      "Found links in 2.html: ['1.html', '3.html']\n",
      "Found links in ai.html: ['inference.html', 'algorithms.html']\n",
      "Found links in tictactoe.html: ['games.html', 'minimax.html']\n",
      "Found links in 3.html: ['2.html', '4.html']\n",
      "Found links in dfs.html: ['bfs.html', 'search.html']\n",
      "Found links in minimax.html: ['search.html', 'games.html']\n",
      "Found links in algorithms.html: ['programming.html', 'recursion.html']\n",
      "Found links in programming.html: ['c.html', 'python.html']\n",
      "Found links in 1.html: ['2.html']\n",
      "Found links in python.html: ['programming.html', 'ai.html']\n",
      "Found links in recursion.html: ['recursion.html']\n",
      "Found links in minesweeper.html: ['games.html']\n",
      "Found links in inference.html: ['ai.html']\n",
      "Found links in 4.html: ['2.html']\n",
      "Found links in games.html: ['tictactoe.html', 'minesweeper.html']\n",
      "Found links in logic.html: ['inference.html']\n",
      "Found links in search.html: ['dfs.html', 'bfs.html', 'minimax.html']\n",
      "Found links in bfs.html: ['search.html']\n",
      "{'c.html': {'programming.html'}, '2.html': {'1.html', '3.html'}, 'ai.html': {'algorithms.html', 'inference.html'}, 'tictactoe.html': {'minimax.html', 'games.html'}, '3.html': {'2.html', '4.html'}, 'dfs.html': {'bfs.html', 'search.html'}, 'minimax.html': {'games.html', 'search.html'}, 'algorithms.html': {'recursion.html', 'programming.html'}, 'programming.html': {'c.html', 'python.html'}, '1.html': {'2.html'}, 'python.html': {'programming.html', 'ai.html'}, 'recursion.html': set(), 'minesweeper.html': {'games.html'}, 'inference.html': {'ai.html'}, '4.html': {'2.html'}, 'games.html': {'minesweeper.html', 'tictactoe.html'}, 'logic.html': {'inference.html'}, 'search.html': {'bfs.html', 'minimax.html', 'dfs.html'}, 'bfs.html': {'search.html'}}\n",
      "PageRank Results from Sampling\n",
      "c.html: 0.0344\n",
      "2.html: 0.2186\n",
      "ai.html: 0.0457\n",
      "tictactoe.html: 0.0248\n",
      "3.html: 0.1140\n",
      "dfs.html: 0.0212\n",
      "minimax.html: 0.0305\n",
      "algorithms.html: 0.0263\n",
      "programming.html: 0.0615\n",
      "1.html: 0.1140\n",
      "python.html: 0.0344\n",
      "recursion.html: 0.0189\n",
      "minesweeper.html: 0.0248\n",
      "inference.html: 0.0349\n",
      "4.html: 0.0608\n",
      "games.html: 0.0465\n",
      "logic.html: 0.0088\n",
      "search.html: 0.0503\n",
      "bfs.html: 0.0295\n",
      "\n",
      "PageRank Results from Iteration\n",
      "c.html: 0.0375\n",
      "2.html: 0.0920\n",
      "ai.html: 0.0569\n",
      "tictactoe.html: 0.0430\n",
      "3.html: 0.0470\n",
      "dfs.html: 0.0295\n",
      "minimax.html: 0.0478\n",
      "algorithms.html: 0.0321\n",
      "programming.html: 0.0694\n",
      "1.html: 0.0470\n",
      "python.html: 0.0374\n",
      "recursion.html: 0.0215\n",
      "minesweeper.html: 0.0430\n",
      "inference.html: 0.0388\n",
      "4.html: 0.0279\n",
      "games.html: 0.0831\n",
      "logic.html: 0.0079\n",
      "search.html: 0.0764\n",
      "bfs.html: 0.0421\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import copy\n",
    "\n",
    "DAMPING = 0.85\n",
    "SAMPLES = 10000\n",
    "\n",
    "def crawl(directory):\n",
    "    pages = dict()\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".html\"):\n",
    "            continue\n",
    "        with open(os.path.join(directory, filename)) as f:\n",
    "            contents = f.read()\n",
    "            links = re.findall(r\"<a\\s+(?:[^>]*?)href=\\\"([^\\\"]*)\\\"\", contents)\n",
    "            print(f\"Found links in {filename}: {links}\")  # Debugging output\n",
    "            pages[filename] = set(links) - {filename}\n",
    "\n",
    "    for filename in pages:\n",
    "        pages[filename] = set(\n",
    "            link for link in pages[filename]\n",
    "            if link in pages\n",
    "        )\n",
    "\n",
    "    return pages\n",
    "\n",
    "def transition_model(corpus, page, damping_factor):\n",
    "    \"\"\"\n",
    "    Return a probability distribution over which page to visit next,\n",
    "    given a current page.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    links = corpus[page]\n",
    "    num_pages = len(corpus)\n",
    "    num_links = len(links)\n",
    "\n",
    "    if links:\n",
    "        for key in corpus:\n",
    "            d[key] = (1 - damping_factor) / num_pages\n",
    "        for key in links:\n",
    "            d[key] += damping_factor / num_links\n",
    "    else:\n",
    "        for key in corpus:\n",
    "            d[key] = 1.0 / num_pages\n",
    "\n",
    "    return d\n",
    "\n",
    "def sample_pagerank(corpus, damping_factor, n):\n",
    "    \"\"\"\n",
    "    Return PageRank values for each page by sampling `n` pages.\n",
    "    \"\"\"\n",
    "    d = {}.fromkeys(corpus.keys(), 0)\n",
    "    page = random.choices(list(corpus.keys()))[0]\n",
    "\n",
    "    for i in range(1, n):\n",
    "        current_dist = transition_model(corpus, page, damping_factor)\n",
    "        for _page in d:\n",
    "            d[_page] = (((i - 1) * d[_page]) + current_dist[_page]) / i\n",
    "        page = random.choices(list(d.keys()), weights=list(d.values()), k=1)[0]\n",
    "\n",
    "    return d\n",
    "\n",
    "def iterate_pagerank(corpus, damping_factor):\n",
    "    \"\"\"\n",
    "    Return PageRank values for each page by iteratively updating\n",
    "    PageRank values until convergence.\n",
    "    \"\"\"\n",
    "    total_pages = len(corpus)\n",
    "    distribution = {}.fromkeys(corpus.keys(), 1.0 / total_pages)\n",
    "    change = True\n",
    "\n",
    "    while change:\n",
    "        change = False\n",
    "        old_distribution = copy.deepcopy(distribution)\n",
    "        for page in corpus:\n",
    "            distribution[page] = ((1 - damping_factor) / total_pages) + \\\n",
    "                (damping_factor * get_sum(corpus, distribution, page))\n",
    "            change = change or abs(\n",
    "                old_distribution[page] - distribution[page]) > 0.001\n",
    "\n",
    "    return distribution\n",
    "\n",
    "def get_sum(corpus, distribution, page):\n",
    "    result = 0\n",
    "    for p in corpus:\n",
    "        if page in corpus[p]:\n",
    "            result += distribution[p] / len(corpus[p])\n",
    "    return result\n",
    "\n",
    "directory = \"site\"  \n",
    "\n",
    "corpus = crawl(directory)\n",
    "print(corpus) \n",
    "\n",
    "ranks_sampled = sample_pagerank(corpus, DAMPING, SAMPLES)\n",
    "print(\"PageRank Results from Sampling\")\n",
    "for page, rank in ranks_sampled.items():\n",
    "    print(f\"{page}: {rank:.4f}\")\n",
    "\n",
    "ranks_iterated = iterate_pagerank(corpus, DAMPING)\n",
    "print(\"\\nPageRank Results from Iteration\")\n",
    "for page, rank in ranks_iterated.items():\n",
    "    print(f\"{page}: {rank:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
